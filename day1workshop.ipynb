{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615123a5-4771-4ad6-ae09-36715a3ce87e",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "\n",
    "I get emails like this a lot:\n",
    "\n",
    ">Do you think we could make a script so that we obtain a average & standard deviation of the iptm+ptm scores from the ranking_debug.json file?  \n",
    "Currently, we calculate the average & standard deviation of the iptm+ptm scores obtained.  It seems that when a prediction is ‘good’, predictions converge on a common solution, iptm+ptm scores are consistently high with low STDEV as shown here: Average 0.61 +/- 0.031  \n",
    "For a ‘poor’ prediction, iptm+ptm scores are low and predictions typically do not converge on a common solution, thus higher STDEV: Average 0.40 +/- 0.059.  \n",
    "It would be nice if we could do this quickly, rather than calculating this by hand.  \n",
    "Doable?\n",
    "\n",
    "Every data analysis tool has certain patterns for common tasks.  In programming languages, we call these patterns *idioms*.  While there are lots of these, you only need a few to go really far in data analysis with python.  This request is a great example of how to combine a few idioms quickly to accomplish a task.  \n",
    "\n",
    "First, we start with how to open a json file and use the `json` library to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5181082-d25f-46c7-aa07-8e07d305e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare where the json file is\n",
    "file_location = 'data/ranking_debug.good.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c8baa-67f3-4387-b5f3-079ef1e64342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the json library\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef749e-72ac-408f-ba99-cf745e852521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The 'with open...' idiom - this is how we open a file in Python.\n",
    "with open(file_location) as f:\n",
    "    # don't call the loaded data 'json'\n",
    "    # if you do, you won't be able to use the json library again!\n",
    "    json_data = json.load(f)  # load the data into a python 'dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc90459-f635-47d9-a688-983a1a42b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the sections of the json file are called\n",
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52e06e-af84-45e5-ae3c-46df22496ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know we want the iptm+ptm data\n",
    "iptm_ptm_data = json_data['iptm+ptm']  # square brackets for accessing dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82f007-de2b-4e2b-b9ce-632e4e7b261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the data look like\n",
    "iptm_ptm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268f4c3-0488-4ef5-aa17-f64d8cbb5c0e",
   "metadata": {},
   "source": [
    "So the data are a list of values paired to an id.  Recall that for 'mathy' lists, we use the `numpy` library, but for data analysis, we use the spreadsheet like library `pandas`.  We'll use `pandas` here.  For single lists in Pandas, we use a `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f260886-1018-47ee-ac80-d9e7ea0c4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.Series(iptm_ptm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ec627-8b25-45eb-ae34-400b60a08420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49cd22-5a90-4bb2-a5e8-0aeecf01f8e2",
   "metadata": {},
   "source": [
    "Remember that Pandas is for data analysis.  So we could ask our series to give us the mean or standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde15e85-326d-47d2-867e-9cf6d1b91c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average:', data.mean(), '+/-', data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78669e-56cb-45d2-8d93-52d5980d9202",
   "metadata": {},
   "source": [
    "... or we could take advantage of Pandas' data analyis tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b835bbc-ac2c-467c-bfe0-55e641e9d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f23431-1f5c-408f-b65f-02ca8f401c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = data.describe()\n",
    "print('Average:', description['mean'], '+/-', description['std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47625f-d12c-421f-ba23-1e0253535781",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Harvest all the lines of code from above to read in the data and print out the average and standard deviation into one cell.  Remember to include the import statements.  Don't include any lines that just printed out information we used to understand what was going on.  \n",
    "\n",
    "Once you're happy, go to the menu on the top and click on *Kernel -> Restart Kernel and Clear all outputs...*, then click *Restart*.  \n",
    "You should then be able to run the cell and have it successfully print out the Average and standard deviation.  You should have a [1] next to your code box once you've run it (indicating that it's the first bit of code to run since the restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feafebd-4a80-44fa-a6b4-676c2f9123fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cfa333-57fc-442c-92f8-793e7b59ae7f",
   "metadata": {},
   "source": [
    "### Exercise pt2:\n",
    "To make them easier to read, Python scripts follow a loose format.  The main thing we want here is to move all the import statements to the top.  Do that now and make sure it runs by restarting the kernel again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef541ed-40d6-44f0-bb02-dd675903ab4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb446a6e-abe3-44e6-b163-e121da08cb29",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "Python has powerful string formatting.  Google \"Python fstrings\" and see if you can format your output a bit prettier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb23aee-201a-49d9-975b-6a12d436a474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d128e1-8183-4746-9b9c-a15bc092a99e",
   "metadata": {},
   "source": [
    "### Make it a script\n",
    "We're most of the way to creating a Python script.  Let's make a useable one now.\n",
    "1. Switch to the *Launcher* tab.  If you don't have one, on the menu on the top, click on *File -> New Launcher*\n",
    "2. Under *Other*, click on *Python File*\n",
    "3. Save the file as `iptm_analysis.py`\n",
    "4. Copy all the code from exercise 2 into the file, then save the file.\n",
    "5. Open a new launcher window and *Terminal* under *Other*\n",
    "6. In the terminal, type in `python iptm_analysis.py` and hit enter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feea6d7-d54d-4e3d-b3dd-106d79017ead",
   "metadata": {},
   "source": [
    "## Make it more versatile\n",
    "The one annoying thing about our script is that we have to change it every time to change the file name.  We *could* do some fancy parsing of the command line to make it a proper Linux-style tool - but that's way overkill.  \n",
    "\n",
    "Python always has the list of commands used to start it stored in the `argv` variable of the `sys` module.  We'll use that as a simple way of making our script better.\n",
    "\n",
    "1. Add `import sys` to the top of your script\n",
    "2. Just below the imports, add `print(sys.argv)`\n",
    "3. Save and run the script again\n",
    "\n",
    "You should get: \n",
    "> `['iptm_analysis.py']`  \n",
    "\n",
    "This means that `sys.argv` is a list of strings from the command line.  Call your script again with `python iptm_analysis.py Weeeee` and see what you get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ffe51-16e1-4560-a494-80e4548c6ae5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Change the print statement in your script so it only prints the second item (*hint: use* `sys.argv[1]`)\n",
    "2. Re-run your script to see if it does what you expect.\n",
    "3. Change the last item on the command line from `Weeee` to `data/ranking_debug.good.json`.\n",
    "4. Re-run your script to see if it does what you expect.\n",
    "5. Set the `file_location` varible to the value from `sys.argv[1]`\n",
    "6. Add `print(file_location)` below where you set `file_location` to `sys.argv[1]`\n",
    "7. Re-run your script to see if it does what you expect.\n",
    "8. Run your script on `data/ranking_debug.poor.json`, see if it does what you expect.\n",
    "9. Celebrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529a064-d55c-431b-97d7-1c1315cf14f2",
   "metadata": {},
   "source": [
    "# Summary statistics\n",
    "We used .mean() and .std(), or the .describe() method, to get the mean and standard deviation of some data.  Let's see how to do some (slighly) more sophisticated analyses, i.e. **t-testing**.  \n",
    "\n",
    "First we need to start with some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fce763-238c-45db-a1f9-a236d193e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "t_test_data = pd.DataFrame({'x'  : [10, 8, 13,9,11,14,6,4,12,7,5],\n",
    "                            'I'  : [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],'I'  : [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],\n",
    "                            'II' : [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74],\n",
    "                            'III': [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],\n",
    "                            # 'IV' : [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89],\n",
    "                           }\n",
    "                          )\n",
    "t_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a055bfa3-df73-4314-99cc-330ac86ff63c",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Find some way to display the mean and standard deviation for all four 'replicates'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c443af-3552-4f52-87f4-da11b5540d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d3c827d-2250-4949-8cc0-a3545a3486a8",
   "metadata": {},
   "source": [
    "... notice anything?  \n",
    "\n",
    "Recall that the t-test requires *Normally* distributed data, ideally the data should have equal variance too, but we can get around that.  `scipy.stats` provides tools to check these things, so let's do that now, at least for the first and third datasets, starting with a test for Normality (called the Shapiro-Wilk test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7f690-3e7b-44fa-a1c9-e66750c43868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "shapiro = stats.shapiro(t_test_data['I'])\n",
    "if shapiro.pvalue < 0.05:\n",
    "    n_string = 'NOT '\n",
    "else:\n",
    "    n_string = ''\n",
    "print(f'The Shapiro-Wilk test for normality gives a p-value of: {shapiro.pvalue:0.3f} for I')\n",
    "print(f'Suggesting that set I IS {n_string}Normally distributed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8b5ce-0453-438f-9e73-ec753fead2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro = stats.shapiro(t_test_data['III'])\n",
    "if shapiro.pvalue < 0.05:\n",
    "    n_string = 'NOT '\n",
    "else:\n",
    "    n_string = ''\n",
    "print(f'The Shapiro-Wilk test for normality gives a p-value of: {shapiro.pvalue:0.3f} for III')\n",
    "print(f'Suggesting that set III IS {n_string}Normally distributed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502f183-84f4-4665-b5b4-8b0bdb308f68",
   "metadata": {},
   "source": [
    "... ok, so III isn't *quite* normally distributed, but 0.026 is really close to 0.05, right?  It's not, like, 0.005 or anything.  I'm sure it's close enough...  \n",
    "\n",
    "Let's check for similar variance using the Bartlett test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fbde7-9090-49f8-8d1e-c76d3bcfa4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bartlett = stats.bartlett(t_test_data['I'], t_test_data['III'])\n",
    "if bartlett.pvalue < 0.05:\n",
    "    n_string = 'NOT '\n",
    "else:\n",
    "    n_string = ''\n",
    "print(f'The Bartlet test for similar variance among samples: {bartlett.pvalue:0.3f} for I and III')\n",
    "print(f'Suggesting that set I and III DO {n_string}have a similar variance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300bf30-b69c-4cdc-8c2d-49b1f6d7ff3a",
   "metadata": {},
   "source": [
    "0.999!  \n",
    "They must both be ok (right?)  We can even test for similar variance in non-Normal data with the Levene test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33b671-026c-4565-898c-c22826ffaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "levene = stats.levene(t_test_data['I'], t_test_data['III'])\n",
    "if levene.pvalue < 0.05:\n",
    "    n_string = 'NOT '\n",
    "else:\n",
    "    n_string = ''\n",
    "print(f'The Levene test for similar variance among samples without assuming Normality: {levene.pvalue:0.3f} for I and III')\n",
    "print(f'Suggesting that set I and III DO {n_string}have a similar variance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af1c0b-fcff-49cc-a884-e891308fd29f",
   "metadata": {},
   "source": [
    "... right!  So - similar variance means they're both good (right?)\n",
    "Finally!  \n",
    "\n",
    "Let's do the t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d6ccf-9e42-44b7-9451-e9dcaa8c6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test = stats.ttest_ind(t_test_data['I'], t_test_data['III'])\n",
    "if t_test.pvalue < 0.05:\n",
    "    n_string = 'NOT '\n",
    "else:\n",
    "    n_string = ''\n",
    "print(f'The t-test has a p-value of: {t_test.pvalue:0.3f} for I and III')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1def1f7-51a3-4852-9b48-6920f2b521ff",
   "metadata": {},
   "source": [
    "WOW!  They MUST be identical, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aac249-1d43-451e-8d19-97f996d701b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plts = fig.subplots(ncols=2)\n",
    "\n",
    "plts[0].scatter(t_test_data['x'], t_test_data['I'])\n",
    "plts[0].set_title('I')\n",
    "plts[1].scatter(t_test_data['x'], t_test_data['III'])\n",
    "plts[1].set_title('III');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f37514-c8a7-436b-87e3-4e32b83fcde0",
   "metadata": {},
   "source": [
    "**Do you think those two datasets are the same?**  \n",
    "\n",
    "*What did we do wrong?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b2ee7-2752-4054-b21b-238a61c53f65",
   "metadata": {},
   "source": [
    "### Aside: Anscombe's Quartet  \n",
    "Francis Anscombe constructed a quartet of datasets with both x- and y-values in 1973 as a demonstration of the importance of looking at your data, along with the effects of outliers and non-Normality.  His goal was to argue against the prevailing sentiment at the time that \"numerical calculations are exact, but graphs are rough.\"\n",
    "\n",
    "The full quartet looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba611e-4f4c-4ea8-a0a1-34bd5cf0c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe_I = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "anscombe_II = [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74]\n",
    "anscombe_III = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "anscombe_IV = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89]\n",
    "\n",
    "anscombe_x = [10, 8, 13,9,11,14,6,4,12,7,5]\n",
    "anscombe_IV_x = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0617a-031f-46bd-a59e-426888ea144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(anscombe_I, [1], showextrema=False)\n",
    "plt.scatter([1]*len(anscombe_I), anscombe_I)\n",
    "plt.violinplot(anscombe_II, [2], showextrema=False)\n",
    "plt.scatter([2]*len(anscombe_II), anscombe_II)\n",
    "plt.violinplot(anscombe_III, [3], showextrema=False)\n",
    "plt.scatter([3]*len(anscombe_III), anscombe_III)\n",
    "plt.violinplot(anscombe_IV, [4], showextrema=False)\n",
    "plt.scatter([4]*len(anscombe_IV), anscombe_IV)\n",
    "plt.ylim(2,14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca00f4f-efe1-4f9d-a442-a46cc13dc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "plts = fig.subplots(ncols=4)\n",
    "plts[0].scatter(anscombe_x, anscombe_I)\n",
    "plts[0].set_title('Anscombe I')\n",
    "plts[1].scatter(anscombe_x, anscombe_II)\n",
    "plts[1].set_title('Anscombe II')\n",
    "plts[2].scatter(anscombe_x, anscombe_III)\n",
    "plts[2].set_title('Anscombe III')\n",
    "plts[3].scatter(anscombe_IV_x, anscombe_IV)\n",
    "plts[3].set_title('Anscombe IV')\n",
    "\n",
    "for p in plts:\n",
    "    p.set_xlim(3,20)\n",
    "    p.set_ylim(2,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec1c26-577a-4ee5-b418-3ad4a076833b",
   "metadata": {},
   "source": [
    "**ALWAYS LOOK AT YOUR DATA!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe8215-8596-4339-9f25-a228d33fe8ea",
   "metadata": {},
   "source": [
    "# Fitting Data\n",
    "Earlier, we played with a simple binding equation.  Let's now use one that's a better representation of experimental data.  We start with the equation for bimolecular $K_{d}$ for the reaction: $Protein + Ligand ⇌ Complex$  \n",
    "\n",
    "$$ K_{d} = \\frac{[Protein][Ligand]}{[Complex]} $$  \n",
    "\n",
    "... but, recall that we only know the initial concentrations $[Protein]_0$ and $[Ligand]_0$, and not the concentrations when we come to equilibrium.  We had to assume that the $Protein$ concentration was very high relative to both the $Ligand$ concentration and $K_d$, and that let us use a simplified version of the equation.  Here's the full equation without those assumptions:\n",
    "\n",
    "$$\\large [Complex] = \\scriptsize \\frac{K_{d} + [ligand]_{0} + [protein]_{0} - \\sqrt{K_{d}^{2} + 2 K_{d} [ligand]_{0} + 2 K_{d} [protein]_{0} + [ligand]_{0}^{2} - 2 [ligand]_{0} [protein]_{0} + [protein]_{0}^{2}}}{2} $$  \n",
    "\n",
    "So you can see why the simplified form is more commonly used:  \n",
    "\n",
    "$$\\large [Complex] = \\normalsize \\frac{[Ligand]}{K_d+[Ligand]}$$  \n",
    "\n",
    "## Exercise\n",
    "The skeleton of a function to calculate the amount of $Complex$ formed is below.\n",
    "1. Fill in the missing part (???) in the first box.\n",
    "2. The second box acts as a check to see if you've done it right.  Run it and see if you get answers that make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead745ea-703c-4334-9f78-a03291c58bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_binding(ligand, kd):\n",
    "    c = ???\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c9d79-e915-408f-9361-7e2da4010531",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand = 0\n",
    "kd = 1\n",
    "print('This should be no complex formation:', simple_binding(ligand, kd))\n",
    "\n",
    "ligand = 0\n",
    "kd = 1000\n",
    "print('This should be no complex formation:', simple_binding(ligand, kd))\n",
    "\n",
    "ligand = 1000\n",
    "kd = 1\n",
    "print('This should be close to 1.0:', simple_binding(ligand, kd))\n",
    "\n",
    "ligand = 1000\n",
    "kd = 1000\n",
    "print('This should be 0.5:', simple_binding(ligand, kd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22584428-bf6e-445a-8fcb-12f3664677cb",
   "metadata": {},
   "source": [
    "Now that we've done some basic checks on our code, we can make some plots.  In order to do so, we need to import a few libraries: `numpy` and `matplotlib`'s `pyplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b95874-dd17-4915-ac92-96f285561876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34305dd-48eb-476a-b04a-d4b0fdfadc3a",
   "metadata": {},
   "source": [
    "Next, we're going to plot the complex formation as a function of ligand concentration, so lets use `numpy` to make an array of ligand concentration values from 1 to 100 (note that we can't use units in `numpy` - we'll just have to be scientists ourselves!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76c879-0a48-49a5-8034-9d365549d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligand_concentrations = np.linspace(0, 10, 5)\n",
    "ligand_concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee371be-7a54-4132-9d79-f2da13cf72a9",
   "metadata": {},
   "source": [
    "Next, we use those ligand concentrations and our function from earlier to get the [complex] formed.  We'll pick an arbitrary $K_d$.  \n",
    "*Note how using a numpy array makes doing lots of math easier!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9761d6-20cf-4423-8978-1afa77387984",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = 1\n",
    "complex_concentrations = simple_binding(ligand_concentrations, kd)\n",
    "complex_concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646d504-7796-4f7f-84bb-52ca2ca543c8",
   "metadata": {},
   "source": [
    "Finally, we plot our binding curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1f430-795d-488d-b35f-d387e58d9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ligand_concentrations, complex_concentrations)\n",
    "plt.title('My first plot (?)')\n",
    "plt.xlabel('$[ligand]$')\n",
    "plt.ylabel('Fraction Bound')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6a3e2-181b-4fe7-924f-c9a7648f5a74",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "That plot looks ugly.  Make a list of 1000 ligand concentrations and re-plot with that.  \n",
    "*Hint: type `np.linspace?` in a cell to see what the parameters for linspace are.  Alternatively, google \"numpy linspace\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe298a-6c54-4fb4-bc17-a535624ff9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9c6e860-dcc5-4689-83e1-e128fe09d7c0",
   "metadata": {},
   "source": [
    "### Exercise (cont.)\n",
    "1. Copy the plotting code from the cell above.\n",
    "2. Save the plot you just made using `plt.savefig('myplot.png')`\n",
    "3. Double-click on the new file in the file brower on the left side of this page.\n",
    "4. Save the figure in another format (e.g. pdf) by changing the extention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8916570-b5af-40a3-b2c5-eb7be0ee4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure as png file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887afa4-5a56-4a9e-8573-dca92944b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure as another format (pdf, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a18df5-8931-4acb-a252-ca80f8f9cd8b",
   "metadata": {},
   "source": [
    "### On to data\n",
    "The Excel spreadsheet `.xlsx` has our data - but how to read it?  *Pandas* to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de83e7-a0bf-4543-800a-e9deb0373173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas requires an additional component called openpyxl to read xls and xlsx files.\n",
    "# Uncomment and run the line below if you get an error about missing openpyxl - you only need to do this once.\n",
    "\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24a7c8-2bb9-42a1-9633-e355a92ceb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = 'data/binding_data.xlsx'\n",
    "\n",
    "import pandas as pd\n",
    "binding_data = pd.read_excel(excel_file)\n",
    "binding_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bbe03-0af9-4752-9b07-b02be3ad65d1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using the lecture notes as an example, make a scatter plot of your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bba04-c370-48d0-9810-bc3eaf588a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e93629-f956-40a4-87e8-48cffeb335b5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Using the lecture notes as an example, import `curve_fit` and fit your data to `simple_binding` function you defined earlier.  \n",
    "2. Calculate the standard deviations.\n",
    "3. Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b4277-ddf3-47f8-b8d2-5201ed871ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51e6bf-1366-4b7e-8437-aa1870a4a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a530d27-b71f-4187-bf94-2951150c6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb4297-123f-4635-81df-2300c1c808a1",
   "metadata": {},
   "source": [
    "### Exercise (cont.)\n",
    "4. Use `np.linspace` to make 1000 ligand concentrations from the minium to maximum values in your Excel file.\n",
    "5. Use the fitted $K_d$ value from `curve_fit`, along with your 1000 concentration values, to plot a 'fitted curve'\n",
    "6. Add the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfb666-2799-4d1a-94b5-771d2146e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347756f-e7b6-4486-83e6-741d5cfb4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e0739-6f4d-4183-a44c-35921a68c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7066546-760a-463a-8dec-f302e25e1543",
   "metadata": {},
   "source": [
    "### Exercise (bonus)\n",
    "Using the lecture slides as a reference, shade ± two standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda347d-9d95-4427-8ab2-b11fb1f8a1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a19554-50e7-49d0-af1d-9f909f02255f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
